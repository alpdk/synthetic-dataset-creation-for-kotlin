# Synthetic Dataset Creation For Kotlin

## Preparations

### Python packages

For proper work this code will require from user packages:

1. datasets
2. jsonlines
3. peft

For their installation we will run this line in terminal:

```pip install datasets jsonlines peft > /dev/null```

### Kotlinc

Kotlinc required for running code that was generated by LLM.
For setting it up we will run script `setup_kotlinc.py`.

This script has 1 parameter:

1. Github link to kotlinc zip archive. (by
   default: [link](https://github.com/JetBrains/kotlin/releases/download/v2.0.20/kotlin-compiler-2.0.20.zip))

This zip archive will be placed in project directory.

You can run this scrypt like:

1. ```python setup_kotlinc.py```
2. ```python setup_kotlinc.py [link]```

### MXeval

MXeval required for testing generated code.
For setting it up we will run script `setup_mxeval.py`.

This script has 1 parameter:

1. Full path to directory where MXeval will be placed or already placed. (default: `project dir`)

You can run this scrypt like:

1. ```python setup_mxeval.py```
2. ```python setup_mxeval.py [path]```

## Test not fine tuned llm model

Before we will start fine tuning llm we need to check what pass rate it has on our testing dataset.
For that we will run script `generate_model_answer.py`

This script has 2 parameters:

1. Name of the model or local path to it. (default: `ibm-granite/granite-3b-code-base-2k`)
2. Dataset for testing. (default: `jetbrains/Kotlin_HumanEval`)

There are 3 ways to run this scrypt like:

1. ```python generate_model_answers.py```
2. ```python generate_model_answers.py [model name/local path]```
3. ```python generate_model_answers.py [model name/local path] [dataset name]```

As a result it will give us a result that was generated by model for each test.
Results will be placed in file `answer`.

For taking pass rate and taking result of compilation run script `check_kotlinc_code.py`:

This script has 1 parameters:

1. Path to the file for checking. (default: `answer`)

There are 3 ways to run this scrypt like:

1. ```python generate_model_answers.py```
2. ```python generate_model_answers.py [model name/local path]```
3. ```python generate_model_answers.py [model name/local path] [dataset name]```

## Generate synthetic dataset for fine tuning

There we want to generate high quality dataset for llm model fine tuning.
For generation of it we will use scrypt `create_synt_data.py`.
This scrypt take code in python and translate it to kotlin.

This scrypt has 3 parameters:

1. Max count of elements that will be in dataset. (default: `100`)
2. Model name for translation of code. (default: `ibm-granite/granite-3b-code-base-2k`)
3. Dataset name that will be translated. (default: `jinaai/code_exercises`)

There are 4 ways to run this scrypt like:

1. ```python create_synt_data.py```
2. ```python create_synt_data.py [integer count of elements to translate]```
3. ```python create_synt_data.py [integer count of elements to translate] [model name/local path]```
4. ```python create_synt_data.py [integer count of elements to translate] [model name/local path] [dataset name]```

## Fine tuning model and checking it

There we will finetune model on our synthetic dataset.
It will be done by scrypt `finetune_model.py`.

This script has 2 parameters:

1. Model name for fine tuning. (default: `ibm-granite/granite-3b-code-base-2k`)
2. Dataset for fine tuning. (default: `test_dataset.json`)

There are 3 ways to run this scrypt like:

1. ```python finetune_model.py```
2. ```python finetune_model.py [model name/local path]```
3. ```python finetune_model.py [model name/local path] [local path to dataset]```

As a result it save a fine tuned model by name `finetuned_model` in project dir.

For checking result of the model we will use script `generate_model_answers.py`,
but as input parameter give `./finetuned_model` as model:

```python generate_model_answers.py ./finetuned_model```

## Code for default run

This code will run default fine tuning solution.

```angular2html
pip install requiremnets.txt > /dev/null
python setup_kotlinc.py
python setup_mxeval.py
python generate_model_answers.py
python create_synt_data.py
python finetune_model.py
python generate_model_answers.py ./finetuned_model
```

## Results

Pass rate for original model is 0.07.

Fine tuned with 300 steps, 100 dataset size, and 
fine tuned layers `[q_proj, k_proj, v_proj, o_proj]` gave same pass rates 0.14.

Fine tuned with 300 steps, 5/100 dataset size, and 
fine tuned layers `[q_proj, k_proj, v_proj]` gave same pass rates 0.18.

Some test was not solved because of syntax mistakes, it can be solved  

For petter performance can be change:

1. Dataset size:
   Dataset contain small count of tasks of different level, 
   as a result it learn how to do specific task and do not spread its knowledge.
2. Dataset quality and better prompt:
   Quality of dataset very sensitive to prompt and require some checks after generations.
3. Better parameters of fine tuning
   Some features could not to learn because of small count of fine tuning steps,
   more layer could be train (q_proj, k_proj, v_proj and o_proj contains only 30% 
   of model parameters) and others.

