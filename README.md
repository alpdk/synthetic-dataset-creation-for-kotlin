# Synthetic Dataset Creation For Kotlin

## Preparations

### Python packages

For proper work, this code will require from user packages:

1. datasets
2. jsonlines
3. peft

For their installation, we will run this line in terminal:

```pip install requirements.txt```

### Kotlinc

Kotlinc required for running code that was generated by LLM.
For setting it up, we will run script `setup_kotlinc.py`.

This script has 1 parameter:

1. GitHub link to kotlinc zip archive. (by
   default: [link](https://github.com/JetBrains/kotlin/releases/download/v2.0.20/kotlin-compiler-2.0.20.zip))

This zip archive will be placed in project directory.

You can run this script like:

1. ```python setup_kotlinc.py```
2. ```python setup_kotlinc.py [link]```

### MXeval

MXeval required for testing generated code.
For setting it up, we will run script `setup_mxeval.py`.

This script has 1 parameter:

1. Full path to directory where MXeval will be placed or already placed. (default: `project dir`)

You can run this script like:

1. ```python setup_mxeval.py```
2. ```python setup_mxeval.py [path]```

## Test not fine-tuned llm model

Before we will start fine-tuning llm we need to check what pass rate it has on our testing dataset.
For that, we will run script `generate_model_answer.py`

This script has 2 parameters:

1. Name of the model or local path to it. (default: `ibm-granite/granite-3b-code-base-2k`)
2. Dataset for testing. (default: `jetbrains/Kotlin_HumanEval`)

There are 3 ways to run this script like:

1. ```python generate_model_answers.py```
2. ```python generate_model_answers.py [model name/local path]```
3. ```python generate_model_answers.py [model name/local path] [dataset name]```

As a result, it will give us a result that was generated by the model for each test.
Results will be placed in file `answer`.

For taking pass rate and taking result of compilation run script `check_kotlinc_code.py`:

This script has 1 parameter:

1. Path to the file for checking. (default: `answer`)

There are 3 ways to run this script like:

1. ```python generate_model_answers.py```
2. ```python generate_model_answers.py [model name/local path]```
3. ```python generate_model_answers.py [model name/local path] [dataset name]```

## Generate synthetic dataset for fine-tuning

There we want to generate high quality dataset for llm model fine-tuning.
For generation of it, we will use script `create_synt_data.py`.
This script take code in python and translate it to Kotlin.

This script has 3 parameters:

1. Max count of elements that will be in dataset. (default: `100`)
2. Model name for translation of code. (default: `ibm-granite/granite-3b-code-base-2k`)
3. Dataset name that will be translated. (default: `jinaai/code_exercises`)

There are 4 ways to run this script like:

1. ```python create_synt_data.py```
2. ```python create_synt_data.py [integer count of elements to translate]```
3. ```python create_synt_data.py [integer count of elements to translate] [model name/local path]```
4. ```python create_synt_data.py [integer count of elements to translate] [model name/local path] [dataset name]```

## Fine-tuning model and checking it

There, we will fine-tune the model on our synthetic dataset.
It will be done by script `finetune_model.py`.

This script has 2 parameters:

1. Model name for fine-tuning. (default: `ibm-granite/granite-3b-code-base-2k`)
2. Dataset for fine-tuning. (default: `test_dataset.json`)

There are 3 ways to run this script like:

1. ```python finetune_model.py```
2. ```python finetune_model.py [model name/local path]```
3. ```python finetune_model.py [model name/local path] [local path to dataset]```

As a result, it save a fine-tuned model by name `finetuned_model` in project dir.

For checking result of the model, we will use script `generate_model_answers.py`,
but as input parameter, give `./finetuned_model` as model:

```python generate_model_answers.py ./finetuned_model```

## Code for default run

This code will run the default fine-tuning solution locally. Require 12GB GPU at least.

```angular2html
pip install requiremnets.txt > /dev/null
python setup_kotlinc.py
python setup_mxeval.py
python generate_model_answers.py
python create_synt_data.py
python finetune_model.py
python generate_model_answers.py ./finetuned_model
```

Otherwise, it is possible to run part with models through Google colab.
Should be run on T4 at least.

Google colab part:

1. Run all cells in 'Environment setup' section
2. Run cells inside 'Generate base model answers' and save file for testing.
3. Run all next cells and save last result for test.

Local part:

1. Run script `python generate_model_answers.py` with first saved file
2. Run script `python generate_model_answers.py` with second saved file

## Results

Pass rate for original model is 0.07.

Fine-tuned with 25 epochs, 100 dataset size, and
fine-tuned layers `[q_proj, k_proj, v_proj, o_proj]` gave same pass rates 0.14.

Fine-tuned with 25 epochs, 5/100 dataset size, and
fine-tuned layers `[q_proj, k_proj, v_proj]` gave same pass rates 0.18.

Fine-tuned with 30 epochs, 200 dataset size, and
fine-tuned layers `[q_proj, k_proj, v_proj]` gave same pass rates 0.27.

Some test was not solved because of syntax mistakes, it can be solved

For better performance can be changed:

1. Dataset size:
   Dataset contain small count of tasks of different level,
   as a result, it learns how to do specific task and do not spread its knowledge.
2. Dataset quality and better prompt:
   Quality of dataset very sensitive to prompt and require some checks after generations.
3. Better parameters of fine-tuning:
   Some features could not to learn because of small count of fine-tuning steps,
   more layer could be trained (q_proj, k_proj, v_proj and o_proj contains only 30%
   of model parameters) and others.
4. Choose better metric:
   Better metric can make model learn better. For example, can be used CodeBleu,
   RUBY, ROUGE-L and etc.
